{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a library (usually used with Python) developed by Google Brain for training and running statistical machine learning models.\n",
    "It is extremely flexible, making almost no assumptions about your model except that you can represent it as a graph of math operations.\n",
    "As a result, it sees lots of use in developing new machine learning models, as well as efficiently training and running existing models.\n",
    "\n",
    "Advantages of TensorFlow over other frameworks:\n",
    " - Most flexible by far -- can use basically any differentiable equation as a model\n",
    " - Most popular and heavily-developed framework by far, used by numerous companies\n",
    " - Works well with CPUs, GPUs, and TPUs (Tensor Processsing Units), and across lots of devices, including distributed training\n",
    " - Incredible debugging and visualization utilities (TensorBoard and tfdbg)\n",
    " - Graph compilation can optimize your code for you\n",
    " - Gives you lots of control over hardware (e.g. whether variables live in CPU or GPU memory)\n",
    " - Open-source!\n",
    "\n",
    "Disadvantages:\n",
    " - Graph programming unfamiliar to many people\n",
    " - Hard to search documentation, lots of deprecated and soon-to-be-deprecated APIs (for TensorFlow 1.0, with TensorFlow 2.0 this has changed)\n",
    " - Generally more verbose and difficult to develop models (but we will also learn Keras, a library meant to make building common models in TensorFlow very simple)\n",
    " - Can be slower than other frameworks for certain models (Apache MXNET)\n",
    " - TensorFlow itself doesn't support every kind of statistical model, most notably \"graphical models\" associated with Bayesian statistics (although, see [bayesflow](https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why care?\n",
    "\n",
    "Because it lets you do all of the awesome things machine learning can do:\n",
    " - [Translate between any two human languages](https://code.fb.com/ai-research/laser-multilingual-sentence-embeddings/)\n",
    " - [Play games with superhuman skill](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/)\n",
    " - [Make a picture look like it was painted by Picasso](http://genekogan.com/works/style-transfer/)\n",
    " - [Generate realistic human faces](https://blog.openai.com/glow/) \n",
    " - [See through walls with WiFi](https://news.mit.edu/2018/artificial-intelligence-senses-people-through-walls-0612)\n",
    " - [Have your phone make calls for you](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html)\n",
    " - [Find every tree in the world (shameless self-plug)](https://medium.com/descarteslabs-team/descartes-labs-urban-trees-tree-canopy-mapping-3b6c85c5c9cc)\n",
    " * ...\n",
    " \n",
    " For example, the people in these pictures never existed but were synthesized by a neural network:\n",
    "![Faces generated by neural network](https://research.nvidia.com/sites/default/files/publications/representative_image_512x256.png) \n",
    "\n",
    " (Image credit: [Nvidia Research, Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://research.nvidia.com/publication/2017-10_Progressive-Growing-of))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of TensorFlow API levels\n",
    "![tensorflow api levels](https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png)\n",
    "\n",
    "We'll be focusing on three of these:\n",
    " - Python frontend (or \"operations-level TensorFlow\") lets you build a computational graph by hand for maximum power and flexibility.\n",
    " - Keras to build neural networks quickly and easily.\n",
    " - Datasets API to load and preprocess data efficiently.\n",
    " \n",
    "This covers the two most common tasks: using Keras to build a simple neural network using common kinds of layers (like fully-connected, convolutional, and recurrent layers), and using operations-level TensorFlow to develop a novel model.\n",
    " \n",
    "(Image credit: [Google Developers Blog, Introduction to TensorFlow Datasets and Estimators](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors and tensor values\n",
    "To do machine learning, you _really_ need to understand tensors and the terminology surrounding them.\n",
    "You can (for machine learning, at least) think of tensors as multidimensional arrays.\n",
    "\n",
    "![tensors](./images/tensors_diagram.jpg)\n",
    "\n",
    "In TensorFlow 2, `tf.Tensor` objects act as wrapper classes for _tensor values_, which are numpy arrays.\n",
    "I'll try to stick to the notation **\"tensor\"** to mean `tf.Tensor` objects and **\"tensor value\"** to mean a multidimensional numpy array.\n",
    "The terminology around tensors and tensor values is the same.\n",
    "\n",
    "A tensor consists of a number of **scalars** organized in some \"rectangular\" way.\n",
    "Scalars have a [**data type**](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType), usually a real floating-point number (`tf.float32`), integer (`tf.int32`), or boolean (`tf.bool`).\n",
    "To access a particular scalar in a tensor, provide an index in each of its **axes** (also called **dimensions**).\n",
    "The total number of axes a tensor has is its **rank**, and each axis has a fixed **size** which determines how many sub-tensors (of rank one less than the parent) you can get by indexing into just that axis.\n",
    "The total number of scalars a tensor can hold is the product of the sizes of its axes (unless it's rank-zero, then it holds a single value).\n",
    "\n",
    "All of the \"size\" properties of a tensor are encapsulated by its **shape**, which fully specifies how many axes it has, what order they come in, and what their sizes are.\n",
    "A tensor's shape is given by a tuple.\n",
    "For example:\n",
    " - a tensor with shape (3) is a vector (rank-1 tensor) with 3 elements\n",
    " - a tensor with shape (3, 4) is a matrix (rank-2 tensor) with 3 rows and 4 columns\n",
    " - a tensor with shape () is a scalar (rank-0 tensor; you need no indices to uniquely identify its only element)\n",
    " - a tensor with shape (2, 2, 2) (rank-3 tensor) is a 2x2x2 \"data cube\"\n",
    " - a tensor with shape (3, 1) is a matrix (rank-2 tensor) with 3 rows and 1 column; subtle difference from having shape (3)\n",
    " \n",
    "When you provide an index for $n$ axes of a rank-$r$ tensor, you get back a tensor with rank $r - n$, and a shape the same as the original but with the indexed axes removed.\n",
    "Numpy and TensorFlow have roughly the same notation for indexing into tensors to obtain sub-tensors.\n",
    "Similarly, some operations called **reductions** operate across one or more axes of a tensor, collapsing those axes into a scalar summarizing their elements.\n",
    "For instance, `tf.reduce_sum` applied to every axis of a tensor outputs a scalar equal to the sum of its scalars.\n",
    "`tf.reduce_mean` applied to axis 0 of a rank-2 tensor returns a vector that, for each column, contains a scalar of the mean of all of the rows in that column. \n",
    "\n",
    "Matrix transposition (which swaps the rows and columns of a matrix) has a natural extension to tensors: it just reorders their axes.\n",
    "You can \"reshape\" a tensor into a _compatible_ shape, which is to say one where the total number of scalars remains the same.\n",
    "\n",
    "I highly recommend understanding numpy indexing, including some of the advanced stuff you can do (like broadcasting, masking, and `np.where`) since a lot of this transfers to TensorFlow.\n",
    "\n",
    "For a good guide to numpy indexing, read https://realpython.com/numpy-array-programming/.\n",
    "For more on tensors, you can read https://www.tensorflow.org/guide/tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor values:\n",
      "3 \n",
      "\n",
      "[1 2 3] \n",
      "\n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] \n",
      "\n",
      "[[1 2 3]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples of creating tensor values in numpy\n",
    "print('Tensor values:')\n",
    "print(np.array(3), '\\n')           # A rank-zero tensor with shape ()\n",
    "print(np.array([1, 2, 3]), '\\n')   # A rank-one tensor with shape (3)\n",
    "print(np.array([[1, 2],\n",
    "                 [3, 4]]), '\\n')   # A rank-two tensor with shape (2, 2)\n",
    "print(np.array([[[1, 2],\n",
    "                 [3, 4]],\n",
    "                [[5, 6],\n",
    "                 [7, 8]]]), '\\n')  # A rank-three tensor with shape (2, 2, 2)\n",
    "print(np.array([[1, 2, 3]]), '\\n') # A rank-two tensor with shape (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "(2, 2)\n",
      "() \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of printing tensor shapes\n",
    "print('Tensor shapes:')\n",
    "print(np.array([[1, 2], [3, 4]]).shape)\n",
    "print(np.array(3).shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor rank: 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of printing tensor rank\n",
    "print('Tensor rank:', \n",
    "      np.array([[1, 2], [3, 4]]).ndim,\n",
    "      '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor indexing:\n",
      "a: [[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] \n",
      "\n",
      "a[0, 0, 0] = 1\n",
      "a[0, 0, 1] = 2\n",
      "a[0, 1, 0] = 3\n",
      "a[1, 0, 0] = 5 \n",
      "\n",
      "a[0, 0, :] = [1 2]\n",
      "a[:, 0, :] =\n",
      " [[1 2]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Examples of indexing\n",
    "print('Tensor indexing:')\n",
    "a = np.array([[[1, 2],\n",
    "                 [3, 4]],\n",
    "                [[5, 6],\n",
    "                 [7, 8]]])\n",
    "\n",
    "print('a:', a, '\\n')\n",
    "\n",
    "print('a[0, 0, 0] =', a[0, 0, 0])  # Selecting a particular scalar\n",
    "print('a[0, 0, 1] =', a[0, 0, 1])\n",
    "print('a[0, 1, 0] =', a[0, 1, 0])\n",
    "print('a[1, 0, 0] =', a[1, 0, 0], '\\n')\n",
    "\n",
    "print('a[0, 0, :] =', a[0, 0, :])   # Take the first value on axes 0 and 1, \n",
    "                                    # leaving axis 2 alone\n",
    "print('a[:, 0, :] =\\n', a[:, 0, :]) # Take the first value on axis 2, \n",
    "                                    # leaving axes 0 and 2 alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable programming\n",
    "Differentiable programming is a paradigm in which a program is represented as a **dataflow graph**: _mathematical computations organized as a directed graph_:\n",
    "![a computational graph](https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png)\n",
    "\n",
    "(Image credit: [Chris Olah's blog, Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/))\n",
    "\n",
    "Nodes in the graph are operations, and edges are tensors, which flow between operations (hence the name).\n",
    "When you want to evaluate an operation (say, to get its output tensor), all of the operations that produce tensors it depends on (found by stepping back one layer in the graph) are evaluated, and so on recursively.\n",
    "In this way, you can define a bunch of interconnected computations as a single graph, and when you want to compute a value, TensorFlow will run _only the computations it needs to to compute your value_.\n",
    "During a single evaluation, tensors have fixed values, so TensorFlow will cache that value in order to prevent computing the same thing multiple times.\n",
    "For instance, when evaluating \"e\" in the above graph, the value of \"b\" is computed only once despite being used twice.\n",
    "\n",
    "It's very useful to keep the graph formalism in mind when writing TensorFlow code, and we'll return to the core ideas of differentiable programming again and again in this course.\n",
    "Because you completely define the graph before running it, TensorFlow can also [optimize the graph with the XLA compiler](https://www.tensorflow.org/xla/), doing things like fusing operations and doing clever memory-saving hacks.\n",
    "\n",
    "The key feature that separates differentiable programming from ordinary graph programming is that in differentiable programming, _you only use differentiable operations_.\n",
    "What this means is that you can use the chain rule (see the next lecture, on the Backpropagation algorithm) to compute _the gradient of any value in the graph with respect to any other value in the graph_.\n",
    "This is incredibly powerful!\n",
    "\n",
    "Critically, you can use _gradient-based optimization methods_ to maximize or minimize values in the graph by changing variable tensors earlier in the graph.\n",
    "This is how we train machine learning models using TensorFlow.\n",
    "The most common family of such optimization algorithms is... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "Gradient descent is a simple and powerful algorithm for minimizing differentiable functions.\n",
    "I'll describe it only briefly here because there are [plenty](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent) [of](https://work.caltech.edu/library/101.html) [good](https://hackernoon.com/gradient-descent-aynk-7cbe95a778da) [explanations](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0) [online](https://www.youtube.com/watch?v=jc2IthslyzM).\n",
    "\n",
    "For a differentiable function $f(\\vec{x})$, the gradient $\\nabla_\\vec{x} f$ gives the direction of steepest increase of $f$ -- the direction to move the parameters $\\vec{x}$ that increases $f$ the most, in a small region around the value of $\\vec{x}$. \n",
    "If we want to minimize $f$, we can therefore keep track of a \"current value of $\\vec{x}$\" and repeatedly take small steps in the opposite direction of the gradient.\n",
    "\n",
    "The size of step is the **learning rate**, denoted $\\alpha$.\n",
    "It should be small, or else we'll overshoot the area where we can assume the gradient really gives the direction of steepest increase (where the linear term in the Taylor series of the function no longer dominates).\n",
    "For steep gradients, we take bigger steps because we have a stronger signal of where the parameters $\\vec{x}$ should go.\n",
    "For $\\alpha$ too small, we converge too slowly to a minimum (by taking very many small steps).\n",
    "For $\\alpha$ too large, we may not be able to minimize the function at all.\n",
    "\n",
    "![gradient descent visualization](./images/gradient_descent.jpg)\n",
    "\n",
    "In machine learning, we define a value that quantifies how badly our model fits the data (the \"loss\" or \"training error\"), then use gradient descent to change the parameters of our model (made possible by differentiable programming) to minimize that value.\n",
    "\n",
    "Below is some example code that computes the gradients and applies the updates by hand to minimize $f(x) = x^2$.\n",
    "TensorFlow will do this for you (and one of the problems on the first lab is to minimize a simple function with TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of x: 0.00026561398887587435\n",
      "Final value of f(x): 7.05507910865531e-08\n"
     ]
    }
   ],
   "source": [
    "# An example of minimizing the function f(x) = x^2 with gradient descent\n",
    "def f(x): return x ** 2\n",
    "def grad_f(x): return 2 * x # Gradient of f(x), computed by hand\n",
    "\n",
    "x = 10\n",
    "learning_rate = 0.05\n",
    "x_values = [x]\n",
    "y_values = [f(x)]\n",
    "\n",
    "for step in range(100):\n",
    "    x = x - learning_rate * grad_f(x)\n",
    "    x_values.append(x)\n",
    "    y_values.append(f(x))\n",
    "    \n",
    "plt.figure(figsize=(5, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(101), x_values, 'b-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(101), y_values, 'r-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "print('Final value of x:', x)\n",
    "print('Final value of f(x):', f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the learning rate\n",
    "There's no single learning rate that's guaranteed to work for every problem.\n",
    "When it's too high, the notion that the gradient represents the direction of steepest increase of the function breaks down -- the gradient is just the linear term in the function's Taylor series, and if you take a big step then the nonlinear terms dominate and you can wind up moving in the wrong direction, oscillating, or overshooting the minimum.\n",
    "When it's too low, the steps taken are too small and optimization will be very slow.\n",
    "\n",
    "So, when optimizing a differentiable program, you'll need to tune the learning rate.\n",
    "A good approach for this is:\n",
    " 1. Start out with a learning rate like $10^{-3}$\n",
    " 2. If the function to be minimized is not decreasing over time (looking at plots, print statements, etc), decrease the learning rate. If it is, try increasing it. Use exponential changes here (i.e. multiply or divide by 10).\n",
    " 3. Repeat until you find the largest learning rate that the problem still converges for, and hone in by making smaller changes if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside on local minima \n",
    "Gradient descent is only guaranteed to converge to the global minimum of a _convex_ function.\n",
    "This seems like a problem, since most loss functions we want to minimize are not convex.\n",
    "However, local minima are very rare in high-dimensional space: for a function to be at a local minimum in $n$-dimensional space, it needs to curve upwards on _every_ axis.\n",
    "A $n$-dimensional \"random\" function, at a point, could curve up or down on a given axis each with probability $\\frac{1}{2}$ so a given point has probability $\\propto 2^{-n}$ of being a local minimum.\n",
    "For neural networks, the vector space of possible parameter values can have millions of dimensions.\n",
    "\n",
    "Instead, we usually care about _saddle points_, where the loss function looks locally flat.\n",
    "Modern variants of gradient descent like momentum, adagrad, and Adam approach this problem by using more information than just the gradient to make parameter updates.\n",
    "But, how to handle these points best when training neural networks (and even how much of a problem they are) is still somewhat of an open problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph programming in TensorFlow\n",
    "\n",
    "Python programs that use TensorFlow require building a computational graph by adding tensors and operations to a graph.\n",
    "\n",
    "## The graph\n",
    "Your code is written in Python, but most of your program logic will be in a computational graph.\n",
    "TensorFlow represents this with a `tf.Graph` object.\n",
    "TensorFlow always has a default `Graph`, and while it does support programs with multiple graphs, this usecase is rare and bug-prone.\n",
    "As a result, you'll basically never create `Graph`s by hand -- they'll just store your program in the background.\n",
    "\n",
    "## Tensors and operations\n",
    "To build up a computational graph, you need to add tensors (directed edges) and operations (nodes) to the `Graph`.\n",
    "Operations are represented by `tf.Operation` objects, and tensors are represented by `tf.Tensor` objects.\n",
    "Most functions in TensorFlow are actually `tf.Operation` constructors, which:\n",
    " - Take zero or more tensors, and possibly some parameters, as input\n",
    " - Permanently add a new operation to the `Graph`\n",
    " - Add a directed edge from each operation that created an input tensor to the new operation\n",
    " - Return a new `tf.Tensor` object (some operations do not return a tensor)\n",
    "\n",
    "For example (quoting the [official docs on `tf.Operation`](https://www.tensorflow.org/api_docs/python/tf/Operation)), `c = tf.matmul(a, b)` creates an `Operation` of type \"MatMul\" that takes tensors `a` and `b` as input, and produces `c` as output.\n",
    "Passing `c` to another operation \"wires\" the two together.\n",
    "\n",
    "In TensorFlow 1, `tf.Tensor` objects are not tensor values, they're _just Python objects you use to build a graph_. However this changes in TensorFlow 2, where they wrap around the values themselves, meaning `tf.Tensor` objects reference values. For example, printing `tf.constant(1.0)` will display,\n",
    "```\n",
    "tf.Tensor(1.0, shape=(), dtype=float32)\n",
    "```\n",
    "\n",
    "To reference a only a tensor's value, we can use,\n",
    "```\n",
    "a = tf.constant(1.0)\n",
    "a.numpy() # 1.0\n",
    "```\n",
    "\n",
    "Additionally, we can use the `tf.print` method to print tensors as their tensor values.\n",
    "\n",
    "The simplest operation is [`tf.constant()`](https://www.tensorflow.org/api_docs/python/tf/constant), which just represents a constant value used in the graph.\n",
    "But think through how it works internally: calling `tf.constant()` actually adds a \"Constant\" operation to the graph and returns a tensor, which takes on the same value every time.\n",
    "Using this constant tensor means adding a directed edge from the \"Constant\" operation to wherever the constant tensor is used.\n",
    "\n",
    "\n",
    "Note: almost any time you would use a tensor, you can instead use a \"tensor-like object\": a numpy `ndarray`, a Python list, or a Python scalar.\n",
    "This will just act like a constant with the given value.\n",
    "\n",
    "Note 2: \"operation constructors\" aren't actually legal Python constructors, but they're wrappers to constructors for subclasses of `tf.Operation` that return different objects.\n",
    "\n",
    "Here is a simple example of using tensors and operations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(3)\n",
    "b = tf.constant(0)\n",
    "c = tf.add(a, b) # Note we could also write \"c = a + b\" since \"+\" will reference tf.add\n",
    "\n",
    "print(c)\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "To train a model, you need parameters that persist changes across multiple runs.\n",
    "These are represented by [`tf.Variable` objects](https://www.tensorflow.org/guide/variables), which can be treated as a `tf.Tensor`.\n",
    "You can use the [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) constructor to create a variable. A new variable requires an initial value while a shape and type can be optionally specified.\n",
    "\n",
    "It's worth reading the [short official Variables guide](https://www.tensorflow.org/guide/variables).\n",
    "\n",
    "## GradientTape\n",
    "`tf.GradientTape` allows us to record operations for automatic differentiation. We create a `tf.GradientTape` object using a `with` block:\n",
    "```\n",
    "with tf.GradientTape() as g:\n",
    "    ...\n",
    "```\n",
    "\n",
    "`tf.Tensor` objects are not tracked by default, so we must call the `watch` method to tell `tf.GradientTape` to watch this tensor. We define the operations we want to differentiate inside the block, and call the `gradient` method to access the recorded gradients. Note, we must specify the source with which the target is differentiated with respect to in the `gradient` method.\n",
    "```\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "dy_dx = g.gradient(y, x) # 6.0\n",
    "```\n",
    "\n",
    "`tf.Variable` objects are automatically tracked by `tf.GradientTape`, so in the above example, if we wrote `x = tf.Variable(3.0)`, we would not need to specify `g.watch(x)`. There are two arguments we for the `tf.GradientTape` constructor:\n",
    "```\n",
    "tf.GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "```\n",
    "\n",
    "The `watch_accessed_variables` argument toggles automatic tracking of variables. The `persistent` argument prevents the `tf.GradientTape` object from being garbage-collected after calling the `gradient` method, allowing us to compute multiple gradients. This is useful if we have more than one variable to differentiate:\n",
    "```\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "dz_dx = g.gradient(z, x) # 108.0\n",
    "dy_dx = g.gradient(y, x) # 6.0\n",
    "del g # Allows g to be garbage-collected\n",
    "```\n",
    "\n",
    "Additionally, `tf.GradientTape`s can be nested to calculate higher-order derivatives:\n",
    "```\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    with tf.GradientTape() as gg:\n",
    "        y = x * x\n",
    "    dy_dx = gg.gradient(y, x)  # 6.0\n",
    "d2y_dx2 = g.gradient(dy_dx, x) # 2.0\n",
    "\n",
    "```\n",
    "\n",
    "I recommend reading the [guide on GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape).\n",
    "\n",
    "## Optimizers\n",
    "[Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) automatically change variables in the graph to find values that minimze certain quantities, usually the loss. In TensorFlow 2, we use Keras optimizers, under the alias `tf.optimizers` for `tf.keras.optimizers`.\n",
    "\n",
    "The archetypal optimizer is [`tf.optimizers.SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD), and just about every other optimizer you use will be some variant of it.\n",
    "Optimizers are just Python objects, created through their constructors with some (optional or mandatory) parameters:\n",
    "```\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "```\n",
    "\n",
    "We then call `gradient` on our `tf.GradientTape` and use `apply_gradients` on the optimizer to update our parameters. Note that as with `gradient`, we must specify the source variables with which we differentiate with respect to in `apply_gradients`. The argument to `apply_gradients` must be able to be unpacked so we usually do this by passing in a `zip` object. When this operation is run, all of the variables in the same graph as the minimization operation are updated by the optimizer according to its optimization algorithm (gradient descent or a variant). We usually wrap this in a function we can then call in a \"training loop.\"\n",
    "```\n",
    "def train(optimizer):\n",
    "    with tf.GradientTape() as g:\n",
    "        t = x * y + z # x, y, and z are previously defined `tf.Variable`s\n",
    "    args = [x, y, z] # Source variables to differentiate w.r.t.\n",
    "    grads = g.gradient(t, args) # Gradient of t w.r.t. x, y, and z\n",
    "    optimizer.apply_gradients(zip(grads, args)) # Applying gradients with update x, y, and z\n",
    "    \n",
    "for training_step in range(1000):\n",
    "    train(optimizer)\n",
    "```\n",
    "\n",
    "Alternatively, the [`minimize`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#minimize) method combines `gradient` and `apply_gradients` into one call. The only change is that it takes in a \"loss\" argument, which must be a callable with no arguments, which can easily be done with a `lambda` expression. By using a callable, we no longer need to use `tf.GradientTape` since the quantity to minimized is now supplied by a function. We still must specify the source variables. Note that `minimize` is not appropriate if we wish to process our gradients before applying them. The above example would then become,\n",
    "```\n",
    "def train(optimizer):\n",
    "    with tf.GradientTape() as g:\n",
    "        t = lambda: x ** y + z # x, y, and z are previously defined `tf.Variable`s\n",
    "    args = [x, y, z] # Source variables to differentiate w.r.t.\n",
    "    optimizer.minimize(t, args) # Gets gradients of t w.r.t. x, y, and z then applies them\n",
    "```\n",
    "\n",
    "In fact, in the above example, `t` references the source variables directly, so technically `tf.GradientTape` is not necessary since `tf.Variable`s are tracked automatically, meaning we could write the example as,\n",
    "```\n",
    "def train(optimizer):\n",
    "    t = lambda: x ** y + z\n",
    "    optimizer.minimize(t, [x, y, z])\n",
    "```\n",
    "\n",
    "However, in cases where the expression for the loss to minimized does not explicitly reference all source variables (for example `t = s + z` where `s = x ** y`), `tf.GradientTape` is necessary to track the tensors.\n",
    "\n",
    "Note: we haven't actually talked about backpropagation, the algorithm TensorFlow uses to efficiently compute gradients of values in the graph with respect to variables. More on this next week.\n",
    "\n",
    "## Name scopes and variable names\n",
    "In addition to giving single operations names, you can also use `tf.name_scope('...')` to give all following operations a common prefix.\n",
    "This is very helpful to group operations that form a \"block\" (and you can think of them as forming one, more complex, operation defined in terms of its inputs and outputs), and will also allow you to collapse them into a single node when visualizing the graph (next week). `tf.name_scope`s can also be nested.\n",
    "\n",
    "`tf.Variable` objects can also be given names. It's good documentation for your code, and will make debugging with TensorBoard and tfdbg _infinitely_ easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'this_is_a_name_scope/variable_name:0' shape=() dtype=int32, numpy=1>\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('this_is_a_name_scope'):\n",
    "    print(tf.Variable(1, name='variable_name', shape=(), dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full program example\n",
    "Below is an example of performing the minimization of $f(x) = x^2$ from before with TensorFlow. Note that technically `tf.GradientTape` is optional since `y` references `x` explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minimize_x_squared'):\n",
    "    x = tf.Variable(10, shape=(), dtype=tf.float32)\n",
    "\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.05)\n",
    "\n",
    "    def train(optimizer):\n",
    "        with tf.GradientTape() as g:\n",
    "            # `tf.pow` takes in two tensors and returns a tensor\n",
    "            y = tf.pow(x, tf.constant(2.0))\n",
    "        args = [x]\n",
    "        grads = g.gradient(y, args)\n",
    "        optimizer.apply_gradients(zip(grads, args))\n",
    "        \n",
    "        # The following can replace the entire body of the function:\n",
    "        # optimizer.minimize(lambda: tf.pow(x, tf.constant(2.0)), [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value of x:\t 10\n",
      "Initial value of f(x):\t 100\n",
      "Final value of x:\t 0.000265613984\n",
      "Final value of f(x):\t 7.05507901e-08\n"
     ]
    }
   ],
   "source": [
    "tf.print('Initial value of x:\\t', x)\n",
    "tf.print('Initial value of f(x):\\t', x ** 2)\n",
    "\n",
    "for step in range(100):\n",
    "    train(optimizer)\n",
    "    \n",
    "tf.print('Final value of x:\\t', x)\n",
    "tf.print('Final value of f(x):\\t', x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper graph example\n",
    "This code creates the computational graph pictured above as a TensorFlow graph, showcasing overloading on arithmetic operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 1\n",
      "e = 2\n",
      "e = 9\n"
     ]
    }
   ],
   "source": [
    "def example(a_value, b_value):\n",
    "    a = tf.constant(a_value, dtype=tf.float32) \n",
    "    b = tf.constant(b_value, dtype=tf.float32)\n",
    "    c = a + b # This is shorthand for the operator function tf.add(a, b)\n",
    "    d = b + tf.constant(1.0)\n",
    "    e = c * d\n",
    "    \n",
    "    return e\n",
    "    \n",
    "tf.print('e =', example(1, 0))\n",
    "tf.print('e =', example(0, 1))\n",
    "tf.print('e =', example(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whew! That's all the basics of writing code in TensorFlow. More interesting problems and models to come :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
