{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Mapping with Fully-Connected Networks\n",
    "Blocks of dense layers are really good at memorizing patterns.\n",
    "This week we'll step outside the traditional \"prediction\" regime of machine learning and use a fully-connected network (FCN) to represent a 2-D scene in its weight by interpolating between known points.\n",
    "\n",
    "Throughout, you should be impressed by two (related) properties of dense neural nets:\n",
    " - They have high enough capacity to entirely memorize complex patterns\n",
    " - They are sufficiently nonlinear to build these patterns up from simple inputs (like (x, y) coordinates in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointcloud mapping\n",
    "\n",
    "![full scene](./images/full_scene.png)\n",
    "\n",
    "### The problem\n",
    "Imagine there is a square (2-D) room with four walls and several square and circular objects in it.\n",
    "We put laser distance sensors around the room, which shoot rays that continue until they hit an object or a wall and report the distance, with limited angular and radial resolution.\n",
    "We want to use this limited data to produce a full 2-D reconstruction of the scene, filling in points the sensors haven't observed with likely values (in essence, interpolating).\n",
    "\n",
    "### The data\n",
    "We know that every ray that there's an object at the point each ray hit, and we also know that there's no object between the camera and the contact point.\n",
    "The data is provided as (x, y) pairs, where some points correspond to places we know are on object boundaries (shown in green above) and points we know are not inside objects (shown in red above).\n",
    "\n",
    "These points come in two data files: `data/positives.txt` for points on the edge of objects, and `data/negatives.txt` for points outside of objects.\n",
    "To generate these files, run `make_data.py`.\n",
    "\n",
    "(Check out the code if you want to see how the points are generated, but don't change anything!)\n",
    "\n",
    "### The model\n",
    "We want to store a full description of the scene in the weights of a neural network.\n",
    "To do this, we'll train a FCN to classify (x, y) points by whether or not they are the edge of a wall or object.\n",
    "Then, we can draw the scene by sampling (x, y) pairs and drawing the model's output.\n",
    "So, the network has \"memorized\" where the objects and walls are, and what their shapes are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load the data\n",
    "Load the data using `np.loadtxt()` (default arguments should work), then convert it to a usable format with a feature array and label array, which is 1 if a point is the edge of an object and 0 if it is not.\n",
    "\n",
    "Functions to look at:\n",
    " - `np.loadtxt`\n",
    " - `np.concatenate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Build a data pipeline\n",
    "Set up a `tf.data.Dataset` and a `tf.summary.SummaryWriter`.\n",
    "This time, there's no test set, just a single dataset. You may want to use the `repeat` method on the dataset for the number of desired epochs. If so, call it after caching so as not to store redundant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Build a model graph\n",
    "The model is a simple fully-connected neural network with four hidden layers, each with 64 units, and an output layer that performs binary classification with sigmoid activation.\n",
    "Recall that the input is just a pair (x, y).\n",
    "\n",
    "Functions to look at throughout:\n",
    " - `tf.cast`\n",
    " - `tf.expand_dims`\n",
    " - `tf.squeeze`\n",
    " - arithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Class to make dense layers\n",
    "It's tedious (and bad design) to explicitly write out every variable, matrix multiplication, etc.\n",
    "Instead, it's common to define classes that create one of a structure you expect to repeat in your model.\n",
    "\n",
    "Write a `tf.Module` class called `Dense`, which has its signature and scopes defined as a stub below.\n",
    "When called, it should add variables and operations to the graph which implement a single dense layer.\n",
    "\n",
    "If `do_activation` is True, it should apply ReLU activation to the computed values.\n",
    "To do this, you can use `tf.nn.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "    '''\n",
    "    Creates a dense layer module.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_input: int\n",
    "        Number of features in the input representation.\n",
    "    dim_output: int\n",
    "        Number of features in the output representation.\n",
    "        Equivalently, number of units in this layer.\n",
    "    do_activation: bool\n",
    "        Whether or not to apply ReLU activation.\n",
    "    postfix: string\n",
    "        Postfix on name scopes in this layer.\n",
    "        Used to simplify visualizations.\n",
    "    name: string\n",
    "        Name of layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A tensor representing the activations of this layer.\n",
    "    '''\n",
    "    def __init__(self, dim_input, dim_output, do_activation=True, postfix='', name=None):\n",
    "        super().__init__(name=name)\n",
    "        with tf.name_scope('dense' + postfix):\n",
    "            pass # Define variables here\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        pass # Compute output tensor here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Class to make neural network\n",
    "Use the `Dense` class to make four dense layers contained as fields in a `tf.Module` class called `FeedForward`.\n",
    "Each should have 64 units, the correct input dimensions, and a distinct (and meaningful) postfix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1: Compute logit\n",
    "Use the `Dense` class to make a final dense layer with `dim_output=1` to compute the final logit. You'll want to put this in a method called `logits` which should take in `self` and an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2: Compute class probability for output\n",
    "Use `tf.sigmoid` to compute the probability that the input coordinate is a boundary point.\n",
    "We will not use this for the loss, just for the output. You'll want to put this in the `__call__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3: Trace function\n",
    "Decorate the `__call__` method with `@tf.function` in order to run a model trace for TensorBoard. I have defined this method for you here, but will not in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        pass # Create dense layers here\n",
    "    \n",
    "    def logits(self, x):\n",
    "        pass # Compute logit here\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        pass # Compute probability here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Compute weighted cross-entropy loss\n",
    "There are about 33 times as many non-boundary coordinates as there are boundary coordinates.\n",
    "To prevent the network from just learning to predict that every coordinate is not a boundary, we should weight the loss function to consider boundary pixels more heavily.\n",
    "\n",
    "To do this, use `tf.nn.weighted_cross_entropy_with_logits()` on the logits computed before, plus the input label, and a positive weight of 20.\n",
    "Then, produce the mean loss for the batch with `tf.reduce_mean`. Put this in a `_loss` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.optimizers.SGD` with `learning_rate=2e-3` and `momentum=0.9`).\n",
    "Then write a train method (taking in a model, input, output, and step), that represents one step of training, where you explicitly compute the gradients in a variable called `gradients` (so we can plot them below) and make an operation that applies those gradients to the graph.\n",
    "\n",
    "Note: don't use `optimizer.minimize`, which will result in computing the gradients twice since we already have them from `GradientTape.gradient`. Add a summary scalar to plot loss in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1: Plot variables and gradients\n",
    "You don't need to do this for every model, but it's illustrative to do at least once. I've written the code for this below, with a pattern borrowed from http://matpalm.com/blog/viz_gradient_norms/. Put this code snippet under the same `with` block where you add your loss summary scalar.\n",
    "\n",
    "```\n",
    "for gradient, variable in zip(gradients, model.trainable_variables):\n",
    "    if variable.name and gradient is not None:\n",
    "        tf.summary.histogram('gradients/' + variable.name, tf.norm(gradient), step=i)\n",
    "        tf.summary.histogram('variables/' + variable.name, tf.norm(variable), step=i)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: train the model\n",
    "Same as last time, except there's no test set.\n",
    "You don't need to distinguish between epochs and steps here, but do run a full trace on the first batch to get the model graph in TensorBoard.\n",
    "\n",
    "I used 50 epochs, and got a final cross-entropy loss of about 0.4.\n",
    "You might not need to use this many epochs to get reasonable results, but more will get crisper results.\n",
    "\n",
    "This might take a while on CPUs, so if it's taking too long, think about:\n",
    " - Using Google Colaboratory\n",
    " - Increasing your batch size\n",
    " - Reducing the frequency with which you run summary ops\n",
    " \n",
    "Watching plots, histograms, etc in TensorBoard as the model trains can be fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Plot the model\n",
    "Let's see what the network learned.\n",
    "Generate 100 x values and 100 y values, each in the range (-1, 1).\n",
    "Then compute the value of `probability` (the sigmoid output of your model) using every coordinate in that grid (every pairwise combination of x and y values), and plot a \"map\" of those probabilities using `plt.scatter()` (check out the `c` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: TensorBoard plots\n",
    "This time, we have some more interesting TensorBoard plots than last time.\n",
    "Run TensorBoard, and look at:\n",
    " - The model graph in Graphs\n",
    " - The loss over time in Scalars\n",
    " - The weights, biases, and gradients over time in Distributions and Histograms\n",
    " \n",
    "The loss should decrease over time, and eventually stabilize (with some noise).\n",
    "\n",
    "The gradients for weights and biases may not go to zero, and your weights and biases may not look like they're converging.\n",
    "This is because of \"bias drift\" where multiple possible settings of weights and biases have the same loss value.\n",
    "To fix this, we could add a term to the loss which penalizes the model slightly based on the magnitude of the bias terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Again, but in Keras\n",
    "Make a Keras Sequential model which is equivalent to the one we've trained in TensorFlow.\n",
    "Train it on the same dataset, and again plot its output in the same way. \n",
    "\n",
    "Hints:\n",
    " - This won't take nearly as long as the previous sections\n",
    " - You don't need to use `keras.utils.to_categorical` since this is binary classification.\n",
    " - The appropriate loss function is 'binary_crossentropy'\n",
    " - To implement the same loss weighting, use `class_weight={0: 1.0, 1:20.0}` in `model.fit()`\n",
    " - You can get the outputs of the model over the entire grid at once with `model.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1: Build a model\n",
    "Use `keras.models.Sequential` to build the same FCN as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2: Define an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3: Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4: Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
