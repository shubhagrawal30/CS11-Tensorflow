{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Fully-Connected Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear modeling\n",
    "Logistic regression and linear regression are linear models: the output variable (or the vector of logits, in logistic regression) is a linear combination of the input features.\n",
    "These models can model nonlinear terms in your data, including interactions between the features, but only if you design those terms by hand.\n",
    "\n",
    "By contrast, nonlinear models such as neural networks can \"automatically\" discover interaction terms and nonlinear terms in the input.\n",
    "This lets them represent much more complicated functions, since most interesting problems involve terms too difficult to design by hand.\n",
    "\n",
    "The key distinguishing factor between different kinds of nonlinear models is how those terms are discovered.\n",
    "Very generic models, like RBF SVMs, can represent almost any kind of mapping from input to output, but as a result can't learn \"smart\" features because the only property they assume about the data is that similar inputs map to similar outputs (the \"local smoothness prior\").\n",
    "More \"opinionated\" models, like neural networks, learn a function from a smaller class but can learn more complicated relationships with less data.\n",
    "\n",
    "#### Aside: The \"no free lunch theorem of machine learning\"\n",
    "This theorem roughly says: no machine learning model strictly outperforms any other model on all problems.\n",
    "It's kind of controversial whether this really means anything: sure, you can pick a test set that has nothing to do with the input, and so random choice does better than even mean-fitting.\n",
    "\n",
    "But, the general idea is important for understanding why we'd chose one model over another.\n",
    "No model does better than any other on random functions.\n",
    "Instead, when picking a model, you need to think about what _properties of the real world_ the model does well on.\n",
    "For instance, if you expect classes to be well-separated, then an SVM is a good choice; if you expect neighborhood relationships to be more important, then K-nearest-neighbors might be a good fit.\n",
    "When thinking about models, think about what _priors_ they impose on the functions they learn.\n",
    "We'll justify neural networks by showing that they impose priors we'd reasonably expect problems in the real world to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense layers, mathematically\n",
    "Fully-connected, feedforward neural networks are a special case of neural networks which use only fully-connected (or \"dense\") layers for their hidden layers.\n",
    "\n",
    "### Dense layers\n",
    "A dense layer $L$ computes its output (or _activation_) as\n",
    "$$\n",
    "\\vec{a_L} = f (W_L \\vec{a_{L-1}} + \\vec{b_L})\n",
    "$$\n",
    "where $W_L$ and $\\vec{b_L}$ are the layer's learned weight matrix and bias vector, $a_{L-1}$ is the activation of the previous layer (or is just the input, if $L = 1$), and $f(x)$ is an **activation function** which performs some (nonlinear) transformation on each element of the output vector.\n",
    "You can think of a dense layer with $n$ inputs and $m$ outputs as performing $m$ linear regressions, each from the same set of $n$ input variables, then applying the activation function to each. \n",
    "\n",
    "The activation function is important.\n",
    "A neural network with no activation function (or a linear activation function) is really just linear regression with more steps -- the matrices for each layer multiply together, and the entire network becomes $\\vec{y} = W \\vec{x} + \\vec{b}$, where $\\vec{y}$ is the output vector, $\\vec{x}$ is the vector of input features, and $W$ and $\\vec{b}$ are some weight matrix and bias vector.\n",
    "A network with nonlinear activation functions is instead able to learn to use those activation functions to \"bend\" its input space in the right places and approximate any function.\n",
    "\n",
    "$W$ and $b$ get learned through gradient descent and backpropagation, possible because every operation here is differentiable.\n",
    "So, it's important that the activation function is differentiable.\n",
    "\n",
    "### Output layer\n",
    "The last layer of a feedforward neural network takes the activations of the last hidden layer (the _final representation_) and uses it to produce the output.\n",
    "\n",
    "For regression, this is easy: just perform a linear regression from the final representation to the output.\n",
    "This is equivalent to a dense layer with a linear (identity) activation function.\n",
    "\n",
    "For classification, logistic regression is used instead of linear regression.\n",
    "Binary classification (\"Bernoulli output\") uses the logistic sigmoid as an activation function; classifying input into one of many classes (\"multinoulli output\") uses the softmax function.\n",
    "\n",
    "For a more thorough coverage, check out http://www.deeplearningbook.org/, chapter 6.2.\n",
    "\n",
    "#### Aside: linear activations\n",
    "Using the identity function as an activation in hidden layers isn't actually totally meaningless.\n",
    "You replace the single matrix multiplication of a full linear regression from inputs to outputs with a product of matrices, which might together have fewer parameters than the single large matrix.\n",
    "So, a block of dense layers with linear activation really learns to perform a _factored linear regression_, which acts as a low-rank approximation to a full linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretations of neural networks\n",
    "Neural networks get interpreted in a ton of different ways.\n",
    "Here are a few of my favorites (though there exist more: [manifold deformation](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/), logic circuits, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation learning\n",
    "Prior to deep learning, the dominant approach to machine learning was \"feature engineering\" -- designing features (interactions, nonlinear terms, etc) by hand with the assumption that some simple model of them (linear model, K-NN) will suffice to solve the problem. \n",
    "One way to think about neural networks is that they perform this process automatically, learning representations in their hidden layers and then fitting a simple model (linear regression or logistic regression) to the output data.\n",
    "Hidden layers are sometimes said to perform \"feature extraction\" in that they learn a good set of features for the problem.\n",
    "\n",
    "The last layer is a linear model of the activation of the second-to-last layer, and so the set of hidden layers defines a nonlinear function from the input features to the final representation.\n",
    "We want this function to \"disentangle\" the input, so that a linear model is sufficient to perform the final step.\n",
    "The activations of hidden layers form \"representations\" of the input that contain the same important information, but discards noise and makes the information more easily accessible.\n",
    "\n",
    "This is achieved with the backpropagation algorithm and gradient descent.\n",
    "The gradient of the loss function not only tells the last layer how it should change to fit the data better, but also how the previous layer should change such that the final layer does better. \n",
    " \n",
    "This same reasoning applies within hidden layers.\n",
    "The last hidden layer has the difficult task of giving the output layer a good enough representation to perform the task linearly.\n",
    "This task is easier if the last hidden layer itself has a good representation as input.\n",
    "And so on.\n",
    "That's why we use multiple hidden layers: because each one makes the next one's job easier.\n",
    "\n",
    "Neural networks of sufficient size are **universal function approximators** (and this is a theorem!), able to represent any function from their inputs to their outputs so long as they have at least one hidden layer.\n",
    "Wider networks learn a more rich representation per hidden layer, and deeper networks learn more representations so each can be simpler.\n",
    "\n",
    "It's worth noting: at every step, the hidden layers change so that their activations would make the _current_ version of the final layer do better.\n",
    "But, the final layer is also changing, so we can't necessarily change each layer in just the right way.\n",
    "This problem gets harder with deeper models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical pattern-matching\n",
    "I mentioned before that a model is only as good as the priors it imposes, and how those line up with the real world.\n",
    "Neural networks impose the prior that the patterns they're finding are _hierarchical_ in structure.\n",
    "That is, the output pattern is easy to see if we frame the question in terms of the presence or absence of slightly simpler patterns.\n",
    "Those patterns are easier to find if we frame them in terms of yet simpler patterns, and so on.\n",
    "\n",
    "For example, recognizing a handwritten digit might boil down to which kinds of common pen strokes (e.g. loops near the top or bottom of an image, straight lines, etc) are present.\n",
    "Those strokes are made up of common shapes (curves, corners, angles).\n",
    "And those shapes are made up of common patterns in pixels, which is the input.\n",
    "Each hidden layer then ought to learn more and more abstract representations, in terms of composing simple patterns of lower-level layers.\n",
    "This also implies that a single layer might recognize multiple kinds of patterns, using groups of neurons instead of the whole layer at once.\n",
    "\n",
    "Is this prior reasonable?\n",
    "I think so.\n",
    "Human problem-solving is often based on breaking up problems into simple components or patterns -- physics, for instance, builds up from simple arithmetic to more advanced math, and then to higher-level reasoning, intuition, and experience.\n",
    "There's something interesting going on at each level of the hierarchy, and the whole problem looks much simpler once you frame each level in terms of the previous levels.\n",
    "\n",
    "#### Aside on DenseNet\n",
    "There's a new-ish neural network architecture called DenseNet which lets each layer look at the output of several past layers instead of just the previous layer.\n",
    "This might be interpreted as letting a model reason about several levels of abstraction at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function composition\n",
    "Another (super cool but pretty unconventional) perspective is that each hidden layer (or a small group of hidden layers) learns to perform a simple function mapping its input to its output.\n",
    "From this perspective, the entire network is a functional program like you might write in Haskell, where the functions learned by hidden layers are chained by composition to form a map from input to output.\n",
    "\n",
    "In this view, you can think of the vector spaces that vectors in different levels live in as _types_, like in type theory and functional programming.\n",
    "Blocks of dense layers learn single functions from their input type to their output type.\n",
    "More complicated layers are more interesting, though: convolutional filters, for instance, apply the same function over and over like a programmer might reuse a function instead of copy-pasting logic.\n",
    "Common patterns in developing neural networks correspond to higher-order functions, like maps and folds.\n",
    "\n",
    "I think this view is fascinating.\n",
    "If you want to read more, check out [this blog post](https://colah.github.io/posts/2015-09-NN-Types-FP/) by Chris Olah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of depth and width\n",
    "#### Capacity\n",
    "Model capacity is first and foremost when deciding on how many layers a model should have (depth) and how many units each layer should have (width).\n",
    "Wider layers learn more complicated functions, and more layers means the network composes more functions; both lead to a higher-capacity model, less likely to underfit but more likely to overfit.\n",
    "Roughly, width adds more parameters than depth, since adding one unit to a layer adds a parameter for every output of the previous layer.\n",
    "Neural networks are usually only a good fit for problems where you have _lots_ of data, so modern architectures tend to be very large and high-capacity.\n",
    "\n",
    "#### Depth vs width\n",
    "![Deep models generalize better](./images/depth_vs_width.png)\n",
    "\n",
    "Image source: http://www.deeplearningbook.org/, chapter 6.4.\n",
    "\n",
    "More interesting than determining the number of parameters is the question of where those parameters should go (more layers vs wider layers).\n",
    "Generally, the modern understanding is that deeper networks are both more powerful (higher capacity) and generalize better in practice than networks that spend their parameters on width.\n",
    "This seems impossible from a [bias-variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) perspective!\n",
    "This effect is generally explained as: making a model \"deep and thin\" imposes a strong prior that the function it learns will be a composition of very many simple functions.\n",
    "This appears to be a good fit for many real-world problems.\n",
    "\n",
    "This point is illustrated in the experiment above (performed on a dataset of house address numbers), where Goodfellow et al. plot the test accuracy of a model against its number of parameters and layers, and find that shallow models are more prone to overfitting as they get wider.\n",
    "\n",
    "The use of models with many layers distinguishes modern \"deep learning\" from earlier attempts at applying neural networks, which typically used one or two hidden layers.\n",
    "It also explains (in part) their incredible success on some very difficult problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "### Logistic sigmoid\n",
    "![sigmoid and its derivative](https://cdn-images-1.medium.com/max/1440/1*gkXI7LYwyGPLU5dn6Jb6Bg.png)\n",
    "\n",
    "Traditionally, neural networks used the logistic sigmoid function,\n",
    "$$\\sigma(x) = \\frac{e^x}{e^x + 1},$$ \n",
    "as an activation function for hidden layers.\n",
    "However, it has the serious problem of **saturating** at high and low activations.\n",
    "\n",
    "When computing the gradient update for a parameter update (with backpropagation), we multiply by the derivative of the activation function at that point.\n",
    "When the weights of a layer using sigmoid activation are large, the input to the logistic function are very positive or very negative.\n",
    "In these places, the derivative of the sigmoid function is near zero, and it's said to have _saturated_: that unit won't learn anything more, because its gradient updates (backpropagated through the sigmoid function) will be zero.\n",
    "\n",
    "The maximum value of its derivative is also $1/4$, so as your model gets deeper and you backpropagate through many sigmoid functions, early layers will see lower gradients and learn much slower.\n",
    "\n",
    "It's worth noting that the logistic sigmoid only has a problem with saturating when it's used for hidden units, not for output units.\n",
    "If the model's very wrong in a binary classification, the cross-entropy loss's log function cancels the exponentials in the sigmoid function and the gradients grow linearly.\n",
    "If the model's very right, the gradient updates will go to zero, as they should.\n",
    "\n",
    "### ReLU\n",
    "![relu and its derivative](https://cdn-images-1.medium.com/max/1440/1*g0yxlK8kEBw8uA1f82XQdA.png)\n",
    "\n",
    "ReLU (rectified linear) activation,\n",
    "$$f(x) = \\max(0, x),$$\n",
    "is now much more common than sigmoid activation because of the saturation problem.\n",
    "Any time it \"fires\", it has a significant derivative and applies parameter updates.\n",
    "In addition, it doesn't \"depress\" gradients when you propagate them through many layers like the sigmoid function.\n",
    "They're also faster to compute the activations and derivatives of than other functions.\n",
    "These three properties make ReLU a good default choice for almost every network, especially deep ones. \n",
    "\n",
    "One caveat is that ReLUs that are initialized badly or receive strong parameter updates from high gradients can get pushed to a region where they never activate, and instead always report zero.\n",
    "These are called \"dead ReLUs\" and can never recover, since the derivative of the ReLU function is zero when it doesn't activate.\n",
    "This can make training ReLU networks with high learning rates dangerous, but there are [some solutions to this](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLUs).\n",
    "\n",
    "Image credit (both): [Andrej Karpathy's article on why you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD with momentum\n",
    "![Gradient descent optimization algorithms](http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif)\n",
    "\n",
    "GIF Source: [\"An overview of gradient descent optimization algorithms\"](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "In regions of parameter space that look like \"ravines\" or \"tacos,\" with a downwards trend in one dimension but a much sharper U-shaped curve in the other dimension, standard SGD will follow the stronger gradient signal, which is perpindicular to the dimension it can really make meaningful progress in.\n",
    "\n",
    "A simple modification, called **momentum**, makes steps based on a linear combination of the previous step direction and the current gradient:\n",
    "$$\n",
    "    m_t = \\beta m_{t-1} - \\alpha \\nabla f(w_{t-1}) \\\\\n",
    "    w_t = w_{t-1} + m_t\n",
    "$$\n",
    "where $\\alpha$ is the learning rate, $f$ is the function to optimize, $w$ is the model weights, and $\\beta$ is a hyperparameter called \"momentum\" which determines how strong of an effect previous steps have on the current step.\n",
    "For $\\beta = 0$, this is equivalent to standard SGD.\n",
    "Common settings for $\\beta$ are around 0.9.\n",
    "\n",
    "Momentum helps avoid the issue with ravines, because the algorithm will accumulate momentum in the downwards-curving direction while eventually damping oscillation in the perpindicular (sharp) direction.\n",
    "\n",
    "See the animation above: the red point is standard SGD (which never makes it past this ravine), and the green point is SGD with momentum.\n",
    "\n",
    "To use this optimizer in TensorFlow, add a `momentum` argument to your [`tf.optimizers.SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD), so it will take in a learning rate and a momentum.\n",
    "In exchange for the little bit of extra tuning you have to do (usually none, just leave momentum at 0.9 or 0.99), momentum should give you much faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight initialization and variance scaling\n",
    "Because of saturating sigmoids, dying ReLUs, and generally wanting our hidden layers to recieve sizeable gradient updates early on in training, how we initialize the weights and biases in a layer is way more important than you might think.\n",
    "Initialization can affect whether the network will train at all, how fast it will train, and how well the optimized model will generalize.\n",
    "\n",
    "General principles are:\n",
    " - Weights should never be initialized to zero, to break symmetry\n",
    " - Weights should be randomly initialized to small values centered around zero\n",
    " - Biases should usually be initialized to zero for hidden units\n",
    " - Units with more inputs may by chance receive many signals of the same sign, so they should have smaller weights to prevent a huge input that kills or saturates the neuron early on\n",
    " - Deeper networks are influenced more by initialization\n",
    " - The variance is pretty much all that matters, not whether you pick a normal or uniform distribution\n",
    "\n",
    "Luckily TensorFlow can pick the standard deviation of a weight initializer for you based on its number of inputs and outputs:\n",
    " - For ReLUs, use `tf.initializers.he_normal` or `tf.initializers.he_uniform`.\n",
    " - For sigmoid units, use `tf.initializers.glorot_normal` or `tf.initializers.glorot_uniform`.\n",
    "\n",
    "He initialization and Glorot (or Xavier) initialization differ in variance by a constant, which makes each suited to a different activation function.\n",
    "If you're curious, [read more about it here](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "A very common method of regularizing (non-final) dense layers is dropout.\n",
    "Dropout \"turns off\" a given unit during a step of training at random with probability $p$, and when running the model the unit is on but its activations are multiplied by $p$.\n",
    "$p$ is usually in the range 0.2 to 0.6, where higher values result in more regularization.\n",
    "\n",
    "Dropout is effective both in reducing the capacity of a dense layer and in solving the problem of _co-adaptation_, where later units learn to correct the mistakes of early units and so the early units never learn to correct themselves (which would be better for generalization).\n",
    "With dropout, the unit cannot rely on other units being present, and so must learn representations that are useful in a broad variety of situations.\n",
    "\n",
    "Dropout can also be thought of as making an approximate ensemble by averaging many different, smaller neural networks.\n",
    "This is the intuition behind why multiplying the activation by the probability of a unit being \"on\" works.\n",
    "\n",
    "To use dropout in TensorFlow, use the [`tf.nn.dropout`](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) operation.\n",
    "Note that this tensor should only be used for training, not for inference.\n",
    "To use it in Keras, use `keras.layers.Dropout`, which handles the training/inference behavior for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "[Keras](https://keras.io/) is a very simple API built on TensorFlow, which makes it easy to build and train common kinds of neural networks.\n",
    "It handles most of the annoying stuff (running gradient updates, initializing tensors, etc) for you.\n",
    "Keras is used a lot in practice for how easy it makes building powerful and performant models, but it lacks the flexibility that makes TensorFlow necessary if you want to do model development.\n",
    "Still, if you're building neural nets in practice there's a good chance it'll be able to do what you need.\n",
    "\n",
    "Training and inference in Keras follow these steps:\n",
    " 1. Build a model either as an instance of `keras.models.Sequential` (this week) or `keras.models.Model` (next week), using layers from `keras.layers`\n",
    " 2. Compile the model, which determines the optimizer, loss function, and which metrics to compute during training and testing\n",
    " 3. Train the model with `model.fit()` or `model.fit_generator()`, passing in training data, validation data, epochs, and batch size\n",
    " 4. Run inference using `model.predict()` \n",
    "\n",
    "### Sequential models\n",
    "Keras has two model-building APIs: the simpler Sequential API for models that work \"one layer at a time\" (feedforward), and the Functional API which is more flexible but complicated.\n",
    "\n",
    "The end of this document has a full worked example of model building and training with a Keras sequential model.\n",
    "\n",
    "For more info, read the [Keras sequential model guide](https://keras.io/getting-started/sequential-model-guide/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TensorFlow debugger\n",
    "TensorFlow has an incredible debugging utility.\n",
    "As the models you develop get more complicated, The debugger and TensorBoard become essential for debugging problems like shapes not matching up, `NaN` and `INF` values appearing instead of numbers, and operations that don't act like you expect.\n",
    "\n",
    "Read [the official guide](https://www.tensorflow.org/guide/debugger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More TensorBoard visualizations\n",
    "`tf.summary.histogram`s are summary operations that act like the `tf.summary.scalar`s we looked at last week.\n",
    "Instead of just taking a scalar, though, they take a tensor of arbitrary shape, and turn it into a histogram counting the frequency of values in intervals in the tensor's values.\n",
    "TensorBoard will plot these histograms against iteration, similar to scalars; you can interact with them in the Histograms tab.\n",
    "\n",
    "There are a few interesting things to plot like this:\n",
    " - Hidden layer activations\n",
    " - Weights\n",
    " - Biases\n",
    " \n",
    "The big one, though, is **gradients**, which can tell you what's going on during optimization and help debug problems like vanishing or exploding gradients.\n",
    "To plot them, you need to get the gradients explicitly using `GradientTape.gradient()` instead of `optimizer.minimize()` like before:\n",
    "```\n",
    "optimizer = tf.optimizers.SGD(2e-3, 0.9)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    loss_scalar = ...\n",
    "    \n",
    "gradients = g.gradient(loss_scalar, trainable_vars)\n",
    "\n",
    "# `gradients` now contains a list of gradients for each variable in `trainable_vars`\n",
    "tf.summary.histogram('my_gradient', gradients[0], step=0)\n",
    "\n",
    "# Instead of `optimizer.minimize()`, this becomes your training operation\n",
    "optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "```\n",
    "\n",
    "All `optimizer.minimize()` does is combine `GradientTape.gradient()` and `optimizer.apply_gradients()`.\n",
    "By breaking these steps up, you get direct accesss to the gradient values and you can do some interesting things with them.\n",
    "For instance, \"clipping\" gradients by capping them at a maximum value is common, to prevent huge steps in gradient descent that can saturate units or step outside the range where the first-order approximation to the loss function is good.\n",
    "\n",
    "Note: if you're computing your gradients for plotting, don't use `optimizer.minimize()` (instead of `apply_gradients()`) at all.\n",
    "If you do, it'll recompute certain values you already have, increasing runtime.\n",
    "\n",
    "For more info, check out the [official documentation on histograms](https://www.tensorflow.org/api_docs/python/tf/summary/histogram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Classification with Keras\n",
    "Here's another full worked example, this time using Keras.\n",
    "Since using Keras mostly involves picking layers and plugging in values, it should suffice as an explanation of how to use the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "I'm using a scipy default dataset again: classifying Iris plants into types based on sepal and petal width and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] \n",
      "\n",
      "Input shape: (150, 4)\n",
      "Target shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load data\n",
    "dataset = load_iris()\n",
    "print('Feature names:', dataset.feature_names, '\\n')\n",
    "\n",
    "x_all = dataset.data\n",
    "y_all = dataset.target\n",
    "\n",
    "# Shuffle some features and targets together\n",
    "together = np.concatenate([x_all, np.expand_dims(y_all, axis=1)], \n",
    "                          axis=1)\n",
    "np.random.shuffle(together)\n",
    "x_all = together[:, :-1]\n",
    "y_all = together[:, -1]\n",
    "\n",
    "print('Input shape:', x_all.shape)\n",
    "print('Target shape:', y_all.shape)\n",
    "\n",
    "# Split data into train and test sets\n",
    "n_points = x_all.shape[0]\n",
    "n_features = x_all.shape[1]\n",
    "n_train = int(n_points * 0.7)\n",
    "n_test = n_points - n_train\n",
    "\n",
    "x_train, x_test = np.split(x_all, [n_train], axis=0)\n",
    "y_train, y_test = np.split(y_all, [n_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model\n",
    "Makes a neural network with 2 hidden layers that use ReLU activation, followed by softmax output to do 3-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Create an \"empty\" model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a 16-unit dense layer with no activation function\n",
    "# input_shape only required for the first layer\n",
    "model.add( Dense(16, input_shape=(4,)) )\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add a 4-unit dense layer with relu activation\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "# Output layer, for 3-class classification: \n",
    "# 3 probabilities, produced by softmax activation\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "optimizer = SGD(lr=2e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy',\n",
    "                       'categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.5509 - categorical_accuracy: 0.3238 - categorical_crossentropy: 1.5509 - val_loss: 1.2516 - val_categorical_accuracy: 0.3556 - val_categorical_crossentropy: 1.2516\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1768 - categorical_accuracy: 0.4190 - categorical_crossentropy: 1.1768 - val_loss: 0.9683 - val_categorical_accuracy: 0.6667 - val_categorical_crossentropy: 0.9683\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9420 - categorical_accuracy: 0.6571 - categorical_crossentropy: 0.9420 - val_loss: 0.8788 - val_categorical_accuracy: 0.6667 - val_categorical_crossentropy: 0.8788\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8713 - categorical_accuracy: 0.7048 - categorical_crossentropy: 0.8713 - val_loss: 0.8827 - val_categorical_accuracy: 0.9333 - val_categorical_crossentropy: 0.8827\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8811 - categorical_accuracy: 0.9333 - categorical_crossentropy: 0.8811 - val_loss: 0.8791 - val_categorical_accuracy: 0.9111 - val_categorical_crossentropy: 0.8791\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8754 - categorical_accuracy: 0.8286 - categorical_crossentropy: 0.8754 - val_loss: 0.8452 - val_categorical_accuracy: 0.9111 - val_categorical_crossentropy: 0.8452\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8398 - categorical_accuracy: 0.8571 - categorical_crossentropy: 0.8398 - val_loss: 0.7961 - val_categorical_accuracy: 0.9778 - val_categorical_crossentropy: 0.7961\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7963 - categorical_accuracy: 0.9333 - categorical_crossentropy: 0.7963 - val_loss: 0.7514 - val_categorical_accuracy: 0.8444 - val_categorical_crossentropy: 0.7514\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7579 - categorical_accuracy: 0.8000 - categorical_crossentropy: 0.7579 - val_loss: 0.7189 - val_categorical_accuracy: 0.7111 - val_categorical_crossentropy: 0.7189\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7293 - categorical_accuracy: 0.6857 - categorical_crossentropy: 0.7293 - val_loss: 0.6949 - val_categorical_accuracy: 0.6889 - val_categorical_crossentropy: 0.6949\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7078 - categorical_accuracy: 0.6857 - categorical_crossentropy: 0.7078 - val_loss: 0.6710 - val_categorical_accuracy: 0.7111 - val_categorical_crossentropy: 0.6710\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6818 - categorical_accuracy: 0.7429 - categorical_crossentropy: 0.6818 - val_loss: 0.6484 - val_categorical_accuracy: 0.8222 - val_categorical_crossentropy: 0.6484\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6599 - categorical_accuracy: 0.8095 - categorical_crossentropy: 0.6599 - val_loss: 0.6267 - val_categorical_accuracy: 0.8222 - val_categorical_crossentropy: 0.6267\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6376 - categorical_accuracy: 0.8190 - categorical_crossentropy: 0.6376 - val_loss: 0.6034 - val_categorical_accuracy: 0.8667 - val_categorical_crossentropy: 0.6034\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6166 - categorical_accuracy: 0.9143 - categorical_crossentropy: 0.6166 - val_loss: 0.5822 - val_categorical_accuracy: 0.9778 - val_categorical_crossentropy: 0.5822\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5986 - categorical_accuracy: 0.9714 - categorical_crossentropy: 0.5986 - val_loss: 0.5637 - val_categorical_accuracy: 0.9778 - val_categorical_crossentropy: 0.5637\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5805 - categorical_accuracy: 0.9524 - categorical_crossentropy: 0.5805 - val_loss: 0.5459 - val_categorical_accuracy: 0.9778 - val_categorical_crossentropy: 0.5459\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5628 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.5628 - val_loss: 0.5297 - val_categorical_accuracy: 0.9556 - val_categorical_crossentropy: 0.5297\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5451 - categorical_accuracy: 0.9524 - categorical_crossentropy: 0.5451 - val_loss: 0.5145 - val_categorical_accuracy: 0.8444 - val_categorical_crossentropy: 0.5145\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5273 - categorical_accuracy: 0.9048 - categorical_crossentropy: 0.5273 - val_loss: 0.5009 - val_categorical_accuracy: 0.8444 - val_categorical_crossentropy: 0.5009\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5130 - categorical_accuracy: 0.8381 - categorical_crossentropy: 0.5130 - val_loss: 0.4914 - val_categorical_accuracy: 0.7778 - val_categorical_crossentropy: 0.4914\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5004 - categorical_accuracy: 0.8000 - categorical_crossentropy: 0.5004 - val_loss: 0.4803 - val_categorical_accuracy: 0.7778 - val_categorical_crossentropy: 0.4803\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4879 - categorical_accuracy: 0.7810 - categorical_crossentropy: 0.4879 - val_loss: 0.4677 - val_categorical_accuracy: 0.7778 - val_categorical_crossentropy: 0.4677\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4747 - categorical_accuracy: 0.8000 - categorical_crossentropy: 0.4747 - val_loss: 0.4533 - val_categorical_accuracy: 0.8000 - val_categorical_crossentropy: 0.4533\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4618 - categorical_accuracy: 0.8571 - categorical_crossentropy: 0.4618 - val_loss: 0.4401 - val_categorical_accuracy: 0.8444 - val_categorical_crossentropy: 0.4401\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4507 - categorical_accuracy: 0.9238 - categorical_crossentropy: 0.4507 - val_loss: 0.4287 - val_categorical_accuracy: 0.8667 - val_categorical_crossentropy: 0.4287\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4401 - categorical_accuracy: 0.9143 - categorical_crossentropy: 0.4401 - val_loss: 0.4203 - val_categorical_accuracy: 0.8444 - val_categorical_crossentropy: 0.4203\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4297 - categorical_accuracy: 0.9429 - categorical_crossentropy: 0.4297 - val_loss: 0.4098 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.4098\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4209 - categorical_accuracy: 0.9619 - categorical_crossentropy: 0.4209 - val_loss: 0.4004 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.4004\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4125 - categorical_accuracy: 0.9524 - categorical_crossentropy: 0.4125 - val_loss: 0.3928 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3928\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4047 - categorical_accuracy: 0.9714 - categorical_crossentropy: 0.4047 - val_loss: 0.3836 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3836\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3945 - categorical_accuracy: 0.9619 - categorical_crossentropy: 0.3945 - val_loss: 0.3776 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3776\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3860 - categorical_accuracy: 0.9524 - categorical_crossentropy: 0.3860 - val_loss: 0.3700 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3700\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3780 - categorical_accuracy: 0.9524 - categorical_crossentropy: 0.3780 - val_loss: 0.3627 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3627\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3706 - categorical_accuracy: 0.9524 - categorical_crossentropy: 0.3706 - val_loss: 0.3551 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3606 - categorical_accuracy: 0.9619 - categorical_crossentropy: 0.3606 - val_loss: 0.3436 - val_categorical_accuracy: 0.9556 - val_categorical_crossentropy: 0.3436\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3639 - categorical_accuracy: 0.9619 - categorical_crossentropy: 0.3639 - val_loss: 0.3370 - val_categorical_accuracy: 1.0000 - val_categorical_crossentropy: 0.3370\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3598 - categorical_accuracy: 0.9238 - categorical_crossentropy: 0.3598 - val_loss: 0.3290 - val_categorical_accuracy: 0.9556 - val_categorical_crossentropy: 0.3290\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3393 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.3393 - val_loss: 0.3231 - val_categorical_accuracy: 0.9333 - val_categorical_crossentropy: 0.3231\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3309 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.3309 - val_loss: 0.3182 - val_categorical_accuracy: 0.9333 - val_categorical_crossentropy: 0.3182\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3231 - categorical_accuracy: 0.9714 - categorical_crossentropy: 0.3231 - val_loss: 0.3144 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3144\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3166 - categorical_accuracy: 0.9714 - categorical_crossentropy: 0.3166 - val_loss: 0.3092 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3092\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3099 - categorical_accuracy: 0.9619 - categorical_crossentropy: 0.3099 - val_loss: 0.3015 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.3015\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3038 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.3038 - val_loss: 0.2916 - val_categorical_accuracy: 0.9333 - val_categorical_crossentropy: 0.2916\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2962 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.2962 - val_loss: 0.2882 - val_categorical_accuracy: 0.9333 - val_categorical_crossentropy: 0.2882\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2948 - categorical_accuracy: 0.9714 - categorical_crossentropy: 0.2948 - val_loss: 0.2875 - val_categorical_accuracy: 0.8889 - val_categorical_crossentropy: 0.2875\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2859 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.2859 - val_loss: 0.2723 - val_categorical_accuracy: 0.9556 - val_categorical_crossentropy: 0.2723\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2782 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.2782 - val_loss: 0.2682 - val_categorical_accuracy: 0.9333 - val_categorical_crossentropy: 0.2682\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2718 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.2718 - val_loss: 0.2626 - val_categorical_accuracy: 0.9333 - val_categorical_crossentropy: 0.2626\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2667 - categorical_accuracy: 0.9810 - categorical_crossentropy: 0.2667 - val_loss: 0.2568 - val_categorical_accuracy: 0.9556 - val_categorical_crossentropy: 0.2568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5770038ef0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to one-hot encode the targets\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "model.fit(x_train, y_train_onehot,\n",
    "          epochs=50, batch_size=32,\n",
    "          validation_data=(x_test, y_test_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test set\n",
    "Not strictly required, since we passed it in as a val set.\n",
    "\n",
    "Prints the loss and metrics:\n",
    "[categorical_crossentropy, categorical_accuracy, categorical_crossentropy]\n",
    "\n",
    "categorical_crossentropy prints twice, since it's both the loss and in the list of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 845us/step - loss: 0.2568 - categorical_accuracy: 0.9556 - categorical_crossentropy: 0.2568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2568022906780243, 0.9555555582046509, 0.2568022906780243]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [6.4 3.1 5.5 1.8]\n",
      "Predicted probabilities: [[0.00493052 0.29414403 0.7009254 ]]\n",
      "Predicted class: 2\n",
      "True class label: 2.0\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.array([x_test[0]]), batch_size=1)\n",
    "\n",
    "print('Inputs:', x_test[0])\n",
    "print('Predicted probabilities:', pred)\n",
    "print('Predicted class:', np.argmax(pred))\n",
    "print('True class label:', y_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
