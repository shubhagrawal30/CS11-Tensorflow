{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Reading Sign Language with Convolutional Networks\n",
    "CNNs have revolutionalized basically every problem that takes an image as input, and the simplest of these is image classification.\n",
    "\n",
    "This project involves using TensorFlow and Keras to build a program that recognizes numbers 0-9 in sign language by classifying images of hands signing those digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Download the data\n",
    "The data is available on Kaggle, at https://www.kaggle.com/ardamavi/sign-language-digits-dataset/.\n",
    "You'll need an account to download it; let me know if you can't do this.\n",
    "\n",
    "Make a directory called `data`, then unzip the data files inside that directory.\n",
    "Your final directory structure should contain files:\n",
    " - `.../lab_4_conv_nets/data/X.npy`\n",
    " - `.../lab_4_conv_nets/data/Y.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understand the data\n",
    "I've taken care of loading the data for you.\n",
    "Read through the code (especially comments) so you understand what it does, and check out the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.load('data/X.npy')\n",
    "y_all = np.load('data/Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For whatever reason, the data's labels aren't the actual\n",
    "# numbers depicted. This box fixes that.\n",
    "# Real-world data is usually messy; this is one example.\n",
    "\n",
    "# Maps dataset-provided label to true label\n",
    "label_map = {0:9, 1:0, 2:7, 3:6, 4:1, 5:8, 6:4, 7:3, 8:2, 9:5}\n",
    "\n",
    "# Correct dataset labels\n",
    "for row in range(y_all.shape[0]):\n",
    "    dataset_label = np.where(y_all[row])[0][0]\n",
    "    y_all[row, :] = np.zeros(10)\n",
    "    y_all[row, label_map[dataset_label]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed numpy rng for reproducibility\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Shuffle features and targets together\n",
    "# Credit for this technique to:\n",
    "# https://stackoverflow.com/questions/4601373/\n",
    "# better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(x_all)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(y_all)\n",
    "\n",
    "# Add a dummy channel axis to input images\n",
    "x_all = np.expand_dims(x_all, axis=-1)\n",
    "\n",
    "# Center and rescale data to the range [-1, 1]\n",
    "x_all = x_all - 0.5\n",
    "x_all = x_all * 2\n",
    "\n",
    "# Create a validation set from 30% of the available data\n",
    "n_points = x_all.shape[0]\n",
    "n_test = int(n_points * 0.3)\n",
    "n_train = n_points - n_test\n",
    "x_train, x_test = np.split(x_all, [n_train], axis=0)\n",
    "y_train, y_test = np.split(y_all, [n_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print important shapes in the dataset\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the first example of each digit in the training set.\n",
    "### You don't need to understand the code, just look at the output.\n",
    "\n",
    "# Set up plots\n",
    "plots = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
    "axes_list = plots[1].ravel()\n",
    "\n",
    "for digit in range(10):\n",
    "    axes = axes_list[digit]\n",
    "    axes.set_axis_off()\n",
    "    axes.set_title(digit)\n",
    "  \n",
    "    # Find the index of the first appearance of this digit\n",
    "    idx = np.where(y_train[:, digit] == 1)[0][0]\n",
    "    \n",
    "    # Plot the image\n",
    "    axes.imshow(x_train[idx, :, :, 0],\n",
    "                cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Build a TensorFlow data pipeline\n",
    "Set up any `tf.data.Dataset` and `tf.summary.SummaryWriter` objects you need.\n",
    "\n",
    "I used two `Dataset`s, but there are multiple ways to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Build a model graph\n",
    "We'll be building a fairly \"traditional\" image-processsing CNN: a few layers of 2-D convolutions and max pooling, then flattening, dense layers, and an output layer.\n",
    "\n",
    "Feel free to experiment with the model architecture.\n",
    "With a simple network, expect around 90% accuracy.\n",
    "(Logistic regression gets about 75%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Class to make dense layers\n",
    "Copy the `Dense` class you wrote last week here, since this model will need dense layers too.\n",
    "\n",
    "(In practice, we'd use `tf.layers.Dense`, which does basically the same thing, but using your code from last week gives you more flexibility to do things like plot histograms with minimal extra work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Class to make convolutional layers\n",
    "Write a `tf.Module` class, similar to `Dense`, called `Conv`, which has its signatures and scopes defined as a stub below.\n",
    "When called, it should:\n",
    " 1. Add variables named `filters` and `biases`, of appropriate shapes, to the graph.\n",
    " 2. Compute the 2-D discrete convolution of `filter` over `input_` using `tf.nn.conv2d`, using the correct filter size and strides, and add in the bias.\n",
    " 3. If `do_activation`, apply ReLU activation using `tf.nn.relu`.\n",
    " 4. If `pool_size > 1`, uses `tf.nn.max_pool` to perform max pooling on the width and height axes.\n",
    " 5. Return the activations if `do_activation`, or the pre-activation otherwise.\n",
    " \n",
    "Hints:\n",
    " - Read [the tf.nn.conv2d documentation](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) extensively! It tells you what shapes your variables need to have.\n",
    " - The layer will have one kernel per input channel, and one bias per output channel\n",
    " - The `conv2d` strides should always be 1 in the batch and channel axes\n",
    " - In `max_pool`, the arguments `ksize` and `strides` will be the same\n",
    " - `padding` can be either 'SAME' or 'VALID'. I used 'SAME' for `tf.nn.conv2d` operations and 'VALID' for \n",
    "   `tf.nn.max_pool` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1: Compute image\n",
    "Create a method in your class called `image` that takes in an input. This method will apply the convolution and activation (if applicable) operations, and return the output, but *not* for the pooling operation (which should go in your `__call__` method). This will allow us to visualize the activation maps of various filters throughout training with `tf.summary.image()` by conveniently calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(tf.Module):\n",
    "    '''\n",
    "    Creates a convolutional layer module.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_channels: int\n",
    "        Number of channels in the input representation.\n",
    "    n_filters: int\n",
    "        Number of channels in the output representation.\n",
    "        Equivalently, number of filters in this layer.\n",
    "    filter_size: int\n",
    "        Width and height of each kernel in the layer's filters.\n",
    "    stride: int\n",
    "        Stride to use in the x and y directions for the\n",
    "        convolution operation.\n",
    "    do_activation: bool\n",
    "        Whether or not to apply ReLU activation.\n",
    "    pool_size: int\n",
    "        If > 1, does max pooling of this size to the\n",
    "        width and height axes of the activation.\n",
    "    postfix: string\n",
    "        Postfix on name and variable scopes in this layer.\n",
    "        Used to simplify visualizations.\n",
    "    name: string\n",
    "        Name of layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A tensor representing the activations of this layer.\n",
    "    '''\n",
    "    def __init__(self, input_channels, n_filters, \n",
    "                 filter_size=3, stride=1,\n",
    "                 do_activation=True, pool_size=1,\n",
    "                 postfix='', name=None):\n",
    "        super().__init__(name=name)\n",
    "        with tf.name_scope('conv' + postfix):\n",
    "            pass # Define variables here\n",
    "            \n",
    "    def image(self, x):\n",
    "        pass # Compute image here\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        pass # Compute output tensor here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Class to make convolutional neural network\n",
    "Now for the fun part.\n",
    "Use `Conv` and `Dense` to make your model, called `ConvNet`.\n",
    "\n",
    "Hints:\n",
    " - Try some layers with convolution and max pooling, then flatten (using `tf.reshape`) and add dense layers\n",
    " - The first convolutional layer has 1 input channel\n",
    " - The input is 64x64, so do lots of downsampling with strides and max pooling before you switch to dense layers to prevent having a huge number of parameters. (If you don't downsample at all, use 32 filters in the last convolutional layer, and use 128 units in the first dense layer, there will be 64\\*64\\*32\\*128 = ~17 million parameters in that dense layer alone! The whole model should ideally have less than 1 million parameters.)\n",
    " - If you're having trouble designing a model, try doing it in Keras first and visualizing the shapes and parameters with `model.summary()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1: Compute logits\n",
    "Use `Dense` to make a final dense layer with `dim_output=10` and no activation to compute the final per-class logits. You'll want to return the logits in a method called `logits` which should take in an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2: Compute class probability for output\n",
    "Use `tf.nn.softmax` to compute the class probabilities. We will not use this for the loss, just for the output. You'll want to put this in the `__call__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3: Trace function\n",
    "Create a decorated trace function so we can visualize the model graph in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        pass # Create network here\n",
    "        \n",
    "    def logits(self, x):\n",
    "        pass # Compute logits here\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        pass # Compute probabilities here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Compute cross-entropy loss\n",
    "Use `tf.nn.softmax_cross_entropy_with_logits()` to compute the per-example loss, then `tf.reduce_mean()` to compute the mean loss for the batch. Put this in a `_loss` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.optimizers.SGD` with `learning_rate=1e-3` and `momentum=0.9`) and update your model. Make a train and test method that each represent an iteration of training. Add a summary operation for loss in both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1: Predicted digit and per-batch accuracy\n",
    "The model should predict the digit it assigns the highest probability.\n",
    "Add a tensor which represents what fraction of the batch the model predicted correctly (its accuracy, or average 0/1 loss), and a summary operation for accuracy in both train and test methods.\n",
    "\n",
    "Hint: to get the numerical value from the one-hot encoded label, use `tf.argmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2: Plot images\n",
    "Add an `image=False` argument to your test method. This will allow us to toggle plotting activation maps on TensorBoard. If `image` is true, create a new 1-channel `tf.summary.image()` for each channel of the activation (pre-pooling). Also, do not plot other summaries when this is the case so as not to skew the loss and accuracy plots with the specific image we will use to plot activation maps. Note that each image needs a channel dimension, though it should be 1 here. \n",
    "\n",
    "You will have to use the `image()` method in your `Conv` class to compute the pre-pooled images and plot them. Make sure to supply your convolutional layers with the correct input when plotting these images, e.g. if you're adding a `tf.summary.image` for your third convolutional layer, supply your `image()` method with the post-pooled outputs of the second convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: train the model\n",
    "Same training loop as always, with one small modification: you don't want `tf.summary.image()` operations every batch because it saves images.\n",
    "Run the other summary operations every batch.\n",
    "\n",
    "When plotting images, make sure you feed in a \"batch\" of a single example (the same one every time). This will let you visualize in the TensorBoard Images tab how the activation maps of various filters on that one example change as the network trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: visualization\n",
    "Run TensorBoard, go to the Images tab, and look at how the final activation maps in various layers differ from each other, and (by dragging the slide bar at the top of each) how the activation map of a single filter develops as the network trains.\n",
    "\n",
    "This is what some activation maps of my first-layer, size-5 convolutions look like:\n",
    "\n",
    " **Background**             |  **Edges** \n",
    ":-------------------------:|:-------------------------:\n",
    "![First-layer background activations](./images/background_conv_1.png) | ![First-layer edges activations](./images/edges_conv_1.png)\n",
    " **Curvature**              | **Gradients** \n",
    "![First-layer curvature activations](./images/curvature_conv_1.png) | ![First-layer gradients activations](./images/gradients_conv_1.png)\n",
    "\n",
    "I see some interesting results here.\n",
    "The first and third filters seem to be activating on the background, while the third also appears to activating around the curvature of the hand. The second detects the outside edges of the hand, and the fourth activates for sharp vertical gradients.\n",
    "\n",
    "Second-layer activation maps are a little more abstract, but still mostly make sense:\n",
    "\n",
    " **Background**           | **Edges** \n",
    ":-------------------------:|:-------------------------:\n",
    "![Second-layer background activations](./images/background_conv_2.png) | ![Second-layer edges activations](./images/edges_conv_2.png)\n",
    " **Fingers**              | **Complexity** \n",
    "![Second-layer fingers activations](./images/fingers_conv_2.png) | ![Second-layer complexity activations](./images/complexity_conv_2.png)\n",
    "\n",
    "The fourth one is really interesting, it seems to detect areas of high complexity in the image.\n",
    "\n",
    "My fourth-layer activation maps are too abstract to make any sense of:\n",
    "![Fourth-layer first activations](./images/first_conv_4.png)\n",
    "![Fourth-layer second activations](./images/second_conv_4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
