{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: A Black-Box Adversarial Attack\n",
    "In this lab, we'll carry out an adversarial attack in the **black box** case: we don't have the weights of the network we're trying to trick (the **target model**), and instead we can only give it inputs and see its outputs.\n",
    "\n",
    "Black-box adversarial attacks are based on the fact that adversarial examples are _transferable_: if it tricks one network, it will likely trick another (but less well).\n",
    "So, since we need network weights to make adversarial examples, we train a second model, the **surrogate model**, to act like the target model.\n",
    "To do this, we'll use the target model as an \"oracle\" to make a dataset where the features are from real examples, and the labels are the outputs of the target model on those examples.\n",
    "If the surrogate model learns to output the same probabilities as the target model on a given input, its internal representations are likely similar.\n",
    "Then, when we make an adversarial example for the surrogate model, it should also trick the target model.\n",
    "\n",
    "The end result of this lab should be an image which looks close to some image in the dataset, but tricks the target network into assigning high confidence to some other class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Load the data\n",
    "This uses the same dataset as last time, so just copy the data over here in the same format.\n",
    "The same data loading and preprocessing as before is in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed numpy rng for reproducibility\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Load data\n",
    "x_all = np.load('data/X.npy')\n",
    "y_all = np.load('data/Y.npy')\n",
    "\n",
    "# Maps dataset-provided label to true label\n",
    "label_map = {0:9, 1:0, 2:7, 3:6, 4:1, 5:8, 6:4, 7:3, 8:2, 9:5}\n",
    "\n",
    "# Correct dataset labels\n",
    "for row in range(y_all.shape[0]):\n",
    "    dataset_label = np.where(y_all[row])[0][0]\n",
    "    y_all[row, :] = np.zeros(10)\n",
    "    y_all[row, label_map[dataset_label]] = 1\n",
    "    \n",
    "# Shuffle features and targets together\n",
    "# Credit for this technique to:\n",
    "# https://stackoverflow.com/questions/4601373/\n",
    "# better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(x_all)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(y_all)\n",
    "\n",
    "# Add a dummy channel axis to input images\n",
    "x_all = np.expand_dims(x_all, axis=-1)\n",
    "\n",
    "# Center and rescale data to the range [-1, 1]\n",
    "x_all = x_all - 0.5\n",
    "x_all = x_all * 2\n",
    "\n",
    "# Create a validation set from 30% of the available data\n",
    "n_points = x_all.shape[0]\n",
    "n_test = int(n_points * 0.3)\n",
    "n_train = n_points - n_test\n",
    "x_train, x_test = np.split(x_all, [n_train], axis=0)\n",
    "y_train, y_test = np.split(y_all, [n_train], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Train the black-box model\n",
    "Below, I've written a CNN in Keras to classify images from the dataset, and the code to train it.\n",
    "This will act as the \"black box model.\"\n",
    "\n",
    "Train the model using the code below.\n",
    "It should hit about 95-96% validation accuracy on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten, MaxPool2D\n",
    "\n",
    "inputs = Input(shape=(64, 64, 1))\n",
    "layer = Conv2D(16, 5, strides=(2, 2), activation='relu')(inputs)\n",
    "layer = Conv2D(16, 3, activation='relu')(layer)\n",
    "layer = MaxPool2D()(layer)\n",
    "\n",
    "layer = Conv2D(32, 3, activation='relu')(layer)\n",
    "layer = Conv2D(32, 3, activation='relu')(layer)\n",
    "layer = MaxPool2D()(layer)\n",
    "\n",
    "layer = Conv2D(64, 3, padding='same', activation='relu')(layer)\n",
    "layer = Conv2D(64, 3, padding='same', activation='relu')(layer)\n",
    "layer = MaxPool2D()(layer)\n",
    "\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(128, activation='relu')(layer)\n",
    "probs = Dense(10, activation='softmax')(layer)\n",
    "\n",
    "target_model = Model(inputs, probs)\n",
    "target_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(1e-3)\n",
    "target_model.compile(opt, loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.fit(x_train, y_train, \n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Build an \"oracle dataset\" and data pipeline\n",
    "Now, we'll treat the target model as a black box: pretend we don't have access to its weights or its original input data.\n",
    "In the wild, all we can do is give it inputs and see its outputs.\n",
    "\n",
    "We want to train a surrogate model to act similarly to the target model, so create a fake \"oracle dataset\" where the features are `x_all` and the outputs are the 10-vectors of probability the target model predicts for that input.\n",
    "\n",
    "Then, set up any `tf.data.Dataset` objects you need.\n",
    "In this case we don't have a test set, just a training set. Add `tf.summary.SummaryWriter`s to save logs to `./logs/surrogate` and `./logs/adversarial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Build a surrogate model\n",
    "The surrogate model is designed to act similarly to the target model, so that adversarial examples we create for it will also work on the target model.\n",
    "Feel free to use any architecture you want, but matching the target architecture closely (maybe with a bit more capacity) is a good bet.\n",
    "\n",
    "Copy code from last week's assignment liberally.\n",
    "Most likely, you won't have to write all that much new code in this section.\n",
    "\n",
    "This should involve tensors that:\n",
    " - Act as hidden convolutional and dense layers\n",
    " - Compute the logits and probabilities for a batch of input images\n",
    " - Compute the predicted digit from the probabilities\n",
    " - Compute the mean cross-entropy loss over a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dense and convolutional layers\n",
    "Copy the `Dense` and `Conv` classes you wrote last week here. You will need them to construct the graph of your surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: The rest of the model\n",
    "Write a `tf.Module` class called `Surrogate` that uses your classes from last week to be your surrogate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1: Adversarial noise\n",
    "This is the only part of the model that really differs from last week, and it's a bit tricky.\n",
    "\n",
    "Add a variable to the graph that represents the adversarial noise we'll add to one example.\n",
    "It should be the shape of a single input image and initialized to zeros.\n",
    "In addition, pass (to the `Variable()` constructor) the keyword argument `constraint=lambda x: tf.clip_by_value(x, -0.3, 0.3)`.\n",
    "Every time the variable is updated, it will become the result of the lambda function, which in this case constrains its pixel values to be in the range -0.3 to 0.3.\n",
    "(Equivalently, at every step we re-project the adversarial noise back into a hypercube at the origin with side length 0.6).\n",
    "I found that 0.3 works well, but feel free to change this value -- smaller values will produce less obvious noise, while larger values will produce more successful attacks. \n",
    "\n",
    "Add a boolean `tf.Variable` (default False) named \"use_noise\" to the graph.\n",
    "This will act as a \"switch\" controlling whether a given run uses adversarial noise. Additionally, add the argument `trainable=False` to the constructor, as it makes no sense to update this variable during training.\n",
    "\n",
    "Add a `tf.cond()` operation that, depending on `use_noise`, switches between the input image and the image plus the adversarial noise.\n",
    "This allows us to train the surrogate model without adversarial noise, then enable it when crafting the adversarial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Compute cross-entropy loss\n",
    "Write a `_loss` function to compute the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Surrogate and adversarial loss and gradients\n",
    "Add an optimizer and train method we'll use to train the surrogate model to act like the target model.\n",
    "It should just minimize the cross-entropy loss between the model's predictions and the targets. When `use_noise` is false, we will not have any gradients to update the adversarial noise tensor with because it will not be referenced in our computation. This will cause TensorFlow to throw (very annoying) warning messages about nonexistent gradients. To suppress this, we can use this neat trick (credit [here](https://stackoverflow.com/questions/60022462/how-to-suppress-specific-warning-in-tensorflow-python)) to preprocess our gradients,\n",
    "```\n",
    "gradients = g.gradient(loss, model.trainable_variables)\n",
    "surrogate_optimizer.apply_gradients( \n",
    "                            (grad, var) for (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                             if grad is not None)\n",
    "```\n",
    "\n",
    "When training the surrogate model, we want to change the model parameters to minimize the loss.\n",
    "When creating the adversarial example, we want to do something very different: _change the adversarial noise tensor alone to maximize the loss_.\n",
    "\n",
    "So, add an `adversarial=False` argument to your train method so we can toggle whether or not to train the surrogate model, or the adversarial noise.\n",
    "\n",
    "Define a new adversarial loss which is the negative of the original loss, and a new optimizer to minimize it.\n",
    "When you call `minimize()`, pass in the keyword argument `var_list=[model.noise]`, where \"noise\" is the name of your adversarial noise tensor.\n",
    "This will prevent the optimizer from changing the weights of the model when we optimize the adversarial noise.\n",
    "\n",
    "Add a summary scalar to plot the adversarial loss decreasing. Then, add a summary histogram and a summary image for the adversarial noise tensor, so we can plot it as we're learning the adversarial example.\n",
    "Finally, add a summary image for the output of `cond`, which will be our adversarial example later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Train the surrogate model\n",
    "Train your surrogate model until it hits high accuracy.\n",
    "At the very least it should have 90% accuracy -- I hit 98-100% on the training set.\n",
    "Overfitting isn't really a concern here since we are actually trying to memorize the target model.\n",
    "\n",
    "When the training loop is done, save the model under `./checkpoints/model_surrogate`.\n",
    "We won't be modifying this particular checkpoint any more, it'll contain the fully-trained surrogate model with zero adversarial noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Learn the adversarial example\n",
    "Finally, it's time to create an adversarial example using our surrogate model.\n",
    "Pick an image from the test set (or use the one I picked below), then run a training loop to minimize the adversarial loss (equivalently, maximize the model loss on that example).\n",
    "To do this, use `surrogate_model.use_noise.assign(True)`.\n",
    "This turns on the `use_noise` switch we set before, and overwrites the input image with the image we're going to turn into an adversarial example.\n",
    "\n",
    "When it's done, save it to a new checkpoint, `./checkpoints/model_adversarial`.\n",
    "This new checkpoint should have exactly the same model parameters as before (remember that we're only optimizing the noise), but a nonzero noise tensor.\n",
    "\n",
    "At the end, also save the adversarial example (which will be contained in the result of your `cond`) and the noise tensor to a numpy array so we can use them later.\n",
    "\n",
    "Periodically write the image and histogram summaries so you can look at them in TensorBoard later. \n",
    "\n",
    "You should be able to do this just by setting `adversarial=True` in your train method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick and plot the image we'll turn into an adversarial example\n",
    "idx = 1\n",
    "img = x_test[idx]\n",
    "lbl = y_test[idx]\n",
    "\n",
    "plt.imshow(img[:, :, 0], cmap='gray')\n",
    "plt.title('True label: {}'.format(np.argmax(lbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Visualize the adversarial example\n",
    "Plot the original example, the adversarial example, and the adversarial noise below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Demonstrate that the surrogate model fails on the adversarial example\n",
    "Compute the surrogate model's prediction and probability for the original example and the adversarial example, and compare them. Remember to set the `use_noise` switch to false when computing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Demonstrate that the target model fails on the adversarial example\n",
    "This is the real test -- transferring the learned adversarial example from the surrogate model to the target model.\n",
    "Compute the target model's prediction and probability for the original example and the adversarial example, and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you've carried out a black-box adversarial attack!\n",
    "These kinds of adversarial attacks are a serious concern in the real world, since they can be [made to work when printed on paper](https://blog.openai.com/robust-adversarial-inputs/) and [we're still bad at defending against them](https://blog.openai.com/adversarial-example-research/).\n",
    "\n",
    "Some interesting things to try:\n",
    " - Did the surrogate model and the target model misclassify the adversarial example in the same way? (Mine did)\n",
    " - How subtle can you make the noise while still tricking the target network?\n",
    " - Look at how the adversarial example evolves through training in TensorBoard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
